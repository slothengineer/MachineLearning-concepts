{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1564c82a-6cf3-49de-b9b5-166a6a2e8507",
   "metadata": {},
   "source": [
    "### A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, you can use conditional probability. You can use the formula for conditional probability:\n",
    "\n",
    "               P(A|B) = P(A ∩ B)/P(B)\n",
    "\n",
    "Where:\n",
    "- P(A|B) is the probability of event A occurring given that event B has occurred.\n",
    "- P(A ∩ B) is the probability of both events A and B occurring.\n",
    "- P(B) is the probability of event B occurring.\n",
    "\n",
    "In this case, event A is \"being a smoker,\" and event B is \"using the health insurance plan.\"\n",
    "\n",
    "You are given:\n",
    "- P(B), the probability of using the health insurance plan, which is 70% or 0.70.\n",
    "- P(A ∩ B), the probability of being a smoker and they use health insurance plan, which is 40% or 0.40.\n",
    "\n",
    "Now, you can calculate P(A|B), the probability of both being a smoker and using the health insurance plan:\n",
    "\n",
    "         P(A|B) = P(A ∩ B)/P(B) = 0.40/0.70 = 0.58\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.58, or 58%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5761727-b63e-4901-9e1c-e3fbfc7e8b44",
   "metadata": {},
   "source": [
    "###  What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "The main difference between them lies in the type of data they are designed to handle and the assumptions they make.\n",
    "\n",
    "1. **Bernoulli Naive Bayes**:\n",
    "   - **Data Type**: Bernoulli Naive Bayes is suitable for binary data, where each feature is either present (1) or absent (0). It's commonly used in document classification tasks where the presence or absence of specific words or features is important.\n",
    "   - **Assumption**: It assumes that features are binary and independent, meaning that the presence or absence of one feature does not affect the presence or absence of another feature.\n",
    "   - **Use Case**: It is often used in spam detection, sentiment analysis, and any task where the focus is on binary presence/absence features.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - **Data Type**: Multinomial Naive Bayes is designed for discrete data, typically used with features that represent counts or frequencies. It is commonly used in text classification when dealing with text data represented as word counts or term frequencies.\n",
    "   - **Assumption**: It assumes that features follow a multinomial distribution, which is appropriate for count-based data, such as word counts.\n",
    "   - **Use Case**: It is widely used in document classification, such as categorizing documents into topics or classifying emails based on the frequency of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68255ada-ed3e-4da2-b4e6-3d1d91d18e7e",
   "metadata": {},
   "source": [
    "### How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Dealing with missing values in the context of Bernoulli Naive Bayes:\n",
    "\n",
    "1. **Imputation**: We can impute missing values by assigning them either 0 or 1, depending on our domain knowledge or the characteristics of our data. However, this approach should be taken with caution, as it may introduce bias or incorrect information into our model.\n",
    "\n",
    "2. **Ignore Missing Values**: Another option is to simply ignore the instances with missing values during model training and prediction. This means that any instance with missing values for certain features will not contribute to the probability calculations for those features. However, this approach may lead to a loss of information and reduced model performance if there is a significant amount of missing data.\n",
    "\n",
    "3. **Feature Engineering**: We can create an additional binary feature to explicitly represent the presence or absence of missing values. This way, we can treat missing values as just another category or state of the feature. This approach allows us to retain information about missing data while still adhering to the binary nature of Bernoulli Naive Bayes.\n",
    "\n",
    "4. **Advanced Imputation Techniques**: Depending on our dataset and the nature of the missing data, we can use more advanced imputation techniques, such as mean imputation, mode imputation, or even machine learning-based imputation methods like k-nearest neighbors imputation. These methods can help preserve the distribution of the data and provide better estimates for missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08756ec2-7b23-4465-a7e6-90d9bd450eb3",
   "metadata": {},
   "source": [
    "### Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that is well-suited for handling continuous numerical data that follows a Gaussian (normal) distribution. It is commonly used in machine learning for various classification tasks, including multi-class classification.\n",
    "\n",
    "Gaussian Naive Bayes can handle multi-class classification by using the following approach:\n",
    "\n",
    "1. **Class Probability Estimation**: For each class in multi-class problem, Gaussian Naive Bayes calculates the probability that an input instance belongs to that class. It does this by estimating the class prior probability (the probability that a random instance belongs to a specific class) and the class conditional probability (the likelihood of observing the given features given the class).\n",
    "\n",
    "2. **Maximum a Posteriori (MAP) Estimation**: To make a prediction for a new instance, Gaussian Naive Bayes applies the Maximum a Posteriori (MAP) estimation. It calculates the posterior probability for each class and assigns the input instance to the class with the highest posterior probability. Mathematically, this can be expressed as:\n",
    "\n",
    "            Prediction = arg(max_c∈classes(P(C=c|X)))\n",
    "\n",
    "   Where:\n",
    "   - Prediction is the predicted class label.\n",
    "   - C is the class.\n",
    "   - X is the input feature vector.\n",
    "\n",
    "3. **Gaussian Distribution Assumption**: Gaussian Naive Bayes assumes that the continuous features within each class follow a Gaussian distribution. This means that it models the distribution of feature values using the mean and variance for each class.\n",
    "\n",
    "Gaussian Naive Bayes is particularly useful when we have continuous data and we can reasonably assume that the feature distributions within each class are approximately Gaussian. However, it's essential to keep in mind that the \"naive\" assumption of feature independence may not hold in all real-world scenarios, and the performance of the model can be affected by violations of this assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5963566c-1def-4cd8-89a6-b748a58551e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        19\n",
      "  versicolor       1.00      0.92      0.96        13\n",
      "   virginica       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## code to suppotrt the ans \n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data \n",
    "y = iris.target \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a4e53-775b-4b0e-9761-5749fbb33cde",
   "metadata": {},
   "source": [
    "#### Data preparation: Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "#### Implementation: Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "#### Results: Report the following performance metrics for each classifier: Accuracy, Precision, Recall, F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "270c4d52-e5e7-46c2-b57b-dbef53294899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7986ae6-d0e8-43d2-bdd4-fe3837d11b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6c72815-88fe-4df6-b1a3-a2fcc4250981",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "data = pd.read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bbf0a38-7776-435a-9dc3-ee200bd51953",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7900d750-106b-4f3f-8e1e-07b8cd495f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebd618f8-561d-4dde-bacd-60f5217077ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = ['accuracy', 'precision', 'recall', 'f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b6e4613-ccf2-4879-9dc0-ad0c43d11135",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_classifier = []\n",
    "results_metric = []\n",
    "results_score = []\n",
    "\n",
    "for metric in scoring_metrics:\n",
    "    results_bernoulli[metric] = cross_val_score(bernoulli_nb, X, y, cv=10, scoring=metric).mean()\n",
    "    results_multinomial[metric] = cross_val_score(multinomial_nb, X, y, cv=10, scoring=metric).mean()\n",
    "    results_gaussian[metric] = cross_val_score(gaussian_nb, X, y, cv=10, scoring=metric).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53b3dad1-842e-43db-9adf-8b3b5113bfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Classifier  accuracy  precision    recall        f1\n",
      "0    BernoulliNB  0.883938   0.886962  0.815239  0.848125\n",
      "1  MultinomialNB  0.786350   0.739318  0.721498  0.728291\n",
      "2     GaussianNB  0.821773   0.710373  0.956952  0.813066\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'Classifier': [],\n",
    "}\n",
    "\n",
    "for metric in scoring_metrics:\n",
    "    results[metric] = []\n",
    "\n",
    "for classifier in [bernoulli_nb, multinomial_nb, gaussian_nb]:\n",
    "    classifier_name = str(classifier).split('(')[0]\n",
    "    results['Classifier'].extend([classifier_name])\n",
    "    for metric in scoring_metrics:\n",
    "        scores = cross_val_score(classifier, X, y, cv=10, scoring=metric.lower())\n",
    "        results[metric].append(scores.mean())\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07f0aa-1a97-4b5a-9a5d-013c22026961",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2e6c8d-3d15-435e-9c60-7eb951b6b81a",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Bernoulli Naive Bayes achieved the highest accuracy and F1 score among the three classifiers. It also had good precision and recall scores. This performance suggests that Bernoulli Naive Bayes is well-suited for this binary classification task, where the presence or absence of specific features (e.g., certain words or patterns in emails) is important. The high precision indicates that it correctly classifies a significant portion of the spam emails without many false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d592c00-e2e5-491e-b036-b5efae905c2d",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes:\n",
    "\n",
    "Multinomial Naive Bayes achieved slightly lower accuracy and F1 score compared to Bernoulli Naive Bayes. It also had lower precision and recall. Multinomial Naive Bayes is typically used for tasks where the data is represented as counts or frequencies (e.g., text classification with term frequencies), and it may not be the best choice for binary data like spam classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fc426-007b-4104-8c6e-4a54844cbc2e",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes:\n",
    "\n",
    "Gaussian Naive Bayes performed well in terms of recall but had lower precision compared to Bernoulli Naive Bayes. This means that it identified a high percentage of spam emails (high recall), but it also had more false positives (lower precision). Gaussian Naive Bayes assumes that features follow a Gaussian distribution, which may not be the best fit for binary data like spam classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f737a6-fe23-4a3d-a628-2c7593fce6a3",
   "metadata": {},
   "source": [
    "Limitations:\n",
    "\n",
    "1. Independence Assumption: Naive Bayes assumes that features are conditionally independent given the class label. In real-world data, this assumption is often violated, which can affect the model's performance.\n",
    "\n",
    "2. Sensitivity to Feature Distribution: Each variant of Naive Bayes (Bernoulli, Multinomial, Gaussian) makes specific assumptions about the distribution of features. If these assumptions don't align with the data, the model's performance can suffer.\n",
    "\n",
    "3. Limited Expressiveness: Naive Bayes is a simple and interpretable model, but it may not capture complex relationships in the data as effectively as more advanced models like decision trees or neural networks.\n",
    "\n",
    "4. Imbalanced Data: Naive Bayes can struggle with imbalanced datasets where one class is much more prevalent than the other. This can lead to biased predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f20d86-1cc5-46a1-8781-bfd62c2d8221",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908c311-261f-4d03-bd60-c62c2c1db9df",
   "metadata": {},
   "source": [
    "**Summary of Findings**:\n",
    "\n",
    "**Classifier Performance**:\n",
    "   - Among the three Naive Bayes classifiers (Bernoulli, Multinomial, Gaussian), Bernoulli Naive Bayes performed the best for the spam classification task. It achieved the highest accuracy, precision, recall, and F1 score.\n",
    "   - Multinomial Naive Bayes had slightly lower performance compared to Bernoulli Naive Bayes, while Gaussian Naive Bayes showed strong recall but lower precision.\n",
    "   - Bernoulli Naive Bayes is well-suited for this binary classification task, where the presence or absence of specific features (e.g., words in emails) is crucial.\n",
    "\n",
    "**Suggestions for Future Work**:\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - Explore more advanced feature engineering techniques to improve model performance. For example, consider using natural language processing (NLP) techniques to extract and represent text-based features more effectively.\n",
    "\n",
    "2. **Model Comparison**:\n",
    "   - Compare the performance of Naive Bayes with other machine learning algorithms, such as decision trees, random forests, support vector machines, or neural networks, to determine if there are better models suited for this task.\n",
    "\n",
    "3. **Hyperparameter Tuning**:\n",
    "   - Perform hyperparameter tuning for each Naive Bayes variant to optimize their performance further. This may involve adjusting smoothing parameters or other relevant hyperparameters.\n",
    "\n",
    "4. **Ensemble Methods**:\n",
    "   - Explore ensemble methods like bagging or boosting to combine the strengths of multiple classifiers and potentially improve classification accuracy.\n",
    "\n",
    "5. **Imbalanced Data Handling**:\n",
    "   - Implement techniques to address class imbalance if it exists in the dataset. This could involve oversampling the minority class, undersampling the majority class, or using specialized algorithms for imbalanced data.\n",
    "\n",
    "6. **Evaluate Additional Features**:\n",
    "   - Consider incorporating additional features into the dataset that may enhance the performance of the classifiers. These features could be related to email metadata, sender information, or other contextual information.\n",
    "\n",
    "7. **Model Interpretability**\n",
    "9. **Security and Privacy Measures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bfc0d0-6a44-41a8-a1af-8a3d0f1c9153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
