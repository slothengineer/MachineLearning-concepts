{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5194d9-2258-4a03-a866-8703617dd58d",
   "metadata": {},
   "source": [
    "###  What is the KNN algorithm?\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for classification and regression tasks. It is a simple and intuitive algorithm that is based on the principle of similarity. KNN makes predictions by identifying the K-nearest data points (neighbors) to a given input data point and then classifying or regressing based on the majority or average of the labels of those nearest neighbors.\n",
    "\n",
    "Here's how the KNN algorithm works:\n",
    "\n",
    "1. **Initialization**: Choose a value for K, which represents the number of nearest neighbors to consider.\n",
    "\n",
    "2. **Training**: In the training phase, the algorithm simply stores the feature vectors and their corresponding class labels (for classification) or target values (for regression).\n",
    "\n",
    "3. **Prediction**:\n",
    "   - For classification: When we want to classify a new data point, the algorithm calculates the distances between this data point and all the data points in the training set (typically using metrics like Euclidean distance or Manhattan distance). \n",
    "   - Then, it selects the K-nearest neighbors (the data points with the smallest distances) to the new data point.\n",
    "   - Finally, it assigns a class label to the new data point based on the majority class among its K-nearest neighbors. In other words, the class label is determined by a majority vote of the K-nearest neighbors.\n",
    "\n",
    "   - For regression: Instead of assigning a class label, KNN calculates the average (or weighted average) of the target values of the K-nearest neighbors and assigns this average as the predicted target value for the new data point.\n",
    "\n",
    "4. **Evaluation**: The performance of the KNN algorithm is typically evaluated using metrics such as accuracy (for classification) or mean squared error (for regression).\n",
    "\n",
    "Key considerations when using KNN:\n",
    "- The choice of the value of K can significantly impact the algorithm's performance. Smaller values of K may lead to noise sensitivity, while larger values of K can smooth out decision boundaries but might lead to over-smoothing.\n",
    "- KNN can be computationally expensive, especially when dealing with large datasets, as it requires calculating distances between the test data point and all training data points.\n",
    "- Feature scaling is important in KNN since features with different scales can disproportionately influence distance calculations.\n",
    "\n",
    "KNN is a non-parametric algorithm, meaning it doesn't make any assumptions about the underlying data distribution. It can be used for various types of data and is often used as a baseline algorithm in machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a51efee-c4e9-4559-8046-ea1f56ffd426",
   "metadata": {},
   "source": [
    "###  How do you choose the value of K in KNN?\n",
    "\n",
    "Here are some common methods to help you determine an appropriate value for K:\n",
    "\n",
    "1. **Cross-Validation**: One of the most common approaches is to use cross-validation, typically k-fold cross-validation, to estimate the performance of KNN for different values of K. We can split our dataset into K subsets (folds), use K-1 folds for training, and the remaining fold for testing. Repeat this process K times, each time using a different fold as the test set. Calculate the performance metric (e.g., accuracy for classification or mean squared error for regression) for each K, and then choose the K that gives us the best performance on average across the K folds.\n",
    "\n",
    "2. **Grid Search**: Perform a grid search over a range of K values. We can specify a range of K values to consider (e.g., K = 1, 3, 5, 7, 9, 11, ...) and evaluate the model's performance using cross-validation for each K. This method allows us to systematically explore different K values and select the one that performs best.\n",
    "\n",
    "3. **Rule of Thumb**: In some cases, there are rule-of-thumb guidelines for choosing K. For example, choosing K to be the square root of the number of data points in our dataset is a common heuristic. However, keep in mind that this rule may not always be the best choice, and it's essential to validate it using cross-validation or other methods.\n",
    "\n",
    "4. **Domain Knowledge**: Consider any domain-specific knowledge we may have about our problem. Sometimes, prior knowledge can guide us in selecting an appropriate K value. For instance, if you know that the decision boundaries in our data are relatively smooth,we might opt for a larger K to reduce noise.\n",
    "\n",
    "5. **Experimentation**: Experiment with different K values and observe how they affect our model's performance. Plotting the performance metrics for various K values can provide insights into the behavior of our model and help us make an informed choice.\n",
    "\n",
    "6. **Odd vs. Even K**: In binary classification problems, it's a good practice to choose an odd value for K to avoid ties in the majority voting. Ties can occur when there is an equal number of neighbors from each class, which can lead to ambiguity in the predictions.\n",
    "\n",
    "7. **Consider Data Size**: The size of your dataset can influence the choice of K. If we have a small dataset, using a small K might lead to overfitting, so it's generally better to choose a larger K. Conversely, with a large dataset, a smaller K may be sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1415367d-a793-436e-9ab4-3f80dfa0aa11",
   "metadata": {},
   "source": [
    "### What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "K-Nearest Neighbors (KNN) can be used for both classification and regression tasks. The primary difference between KNN classifier and KNN regressor lies in the type of output they produce and how they handle different types of machine learning problems:\n",
    "\n",
    "1. **KNN Classifier**:\n",
    "   - **Task**: KNN classifier is used for classification tasks, where the goal is to assign a class label (category) to a given data point.\n",
    "   - **Output**: The output of a KNN classifier is a discrete class label. It assigns the class label that is most common among the K-nearest neighbors of the data point being predicted.\n",
    "   - **Use Case**: KNN classification is suitable for problems such as image classification, spam email detection, and sentiment analysis, where the output is a categorical label.\n",
    "\n",
    "2. **KNN Regressor**:\n",
    "   - **Task**: KNN regressor is used for regression tasks, where the goal is to predict a continuous numerical value (target) for a given data point.\n",
    "   - **Output**: The output of a KNN regressor is a numerical value, typically the average (or weighted average) of the target values of the K-nearest neighbors of the data point being predicted. It aims to estimate a real-valued function based on the values of its neighbors.\n",
    "   - **Use Case**: KNN regression is suitable for problems such as house price prediction, stock price forecasting, and temperature prediction, where the output is a continuous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18855ea-a23f-4a64-8206-882b0ff3928a",
   "metadata": {},
   "source": [
    "### How do you measure the performance of KNN?\n",
    "\n",
    "**For KNN Classification**:\n",
    "\n",
    "1. **Accuracy**: Accuracy is the most straightforward metric for classification tasks. It calculates the ratio of correctly classified instances to the total number of instances in the dataset. While accuracy is easy to understand, it may not be the best metric for imbalanced datasets.\n",
    "\n",
    "   Accuracy = (Number of Correctly Classified Instances) / (Total Number of Instances)\n",
    "\n",
    "2. **Precision, Recall, and F1-Score**: These metrics are especially useful when dealing with imbalanced datasets or when you want to assess the trade-off between precision and recall.\n",
    "   - Precision: The ratio of true positives to the total number of predicted positives.\n",
    "   - Recall: The ratio of true positives to the total number of actual positives.\n",
    "   - F1-Score: The harmonic mean of precision and recall, which balances the two metrics.\n",
    "\n",
    "3. **Confusion Matrix**: A confusion matrix provides a detailed breakdown of true positives, true negatives, false positives, and false negatives, helping you understand the model's performance in more depth.\n",
    "\n",
    "**For KNN Regression**:\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**: MAE calculates the average absolute difference between the predicted values and the actual target values. It provides a straightforward measure of the model's prediction error.\n",
    "   \n",
    "   MAE = (1/n) Σ|Actual - Predicted|\n",
    "\n",
    "2. **Mean Squared Error (MSE)**: MSE calculates the average squared difference between the predicted values and the actual target values. It penalizes larger errors more heavily than MAE.\n",
    "\n",
    "   MSE = (1/n) Σ(Actual - Predicted)^2\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**: RMSE is the square root of MSE and provides an interpretable measure of prediction error in the same unit as the target variable. It's sensitive to outliers.\n",
    "\n",
    "   RMSE = √MSE\n",
    "\n",
    "4. **R-squared (R²) or Coefficient of Determination**: R-squared measures the proportion of variance in the target variable that is explained by the model. It ranges from 0 to 1, with higher values indicating a better fit. However, it doesn't account for overfitting.\n",
    "\n",
    "   R² = 1 - (MSE(Model) / MSE(Mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d98c43-2c8d-4fb0-9c1b-f2f93fd087ca",
   "metadata": {},
   "source": [
    "### What is the curse of dimensionality in KNN?\n",
    "\n",
    "The \"curse of dimensionality\" is a term used in machine learning and data science to describe the challenges and issues that arise when working with high-dimensional data, and it can affect the performance of algorithms like K-Nearest Neighbors (KNN). The curse of dimensionality is characterized by several key problems:\n",
    "\n",
    "1. **Increased Computational Complexity**: As the number of dimensions (features) in dataset increases, the computational cost of KNN grows exponentially. This is because calculating distances between data points becomes more computationally intensive in high-dimensional spaces.\n",
    "\n",
    "2. **Sparsity of Data**: In high-dimensional spaces, data points tend to become sparse. Most of the data points are located far away from each other, and there is less data available for making meaningful distance-based decisions. This can lead to a decrease in the effectiveness of KNN, as the \"nearest neighbors\" might not be very close in high-dimensional space.\n",
    "\n",
    "3. **Overfitting**: KNN can become prone to overfitting in high-dimensional spaces. With a large number of dimensions, it becomes easier for the algorithm to find close neighbors that are not actually similar in terms of the underlying data distribution. This can lead to poor generalization on new, unseen data.\n",
    "\n",
    "4. **Loss of Discriminatory Power**: High-dimensional data can make it challenging to distinguish between different data points. When many features are included, some of them may be irrelevant or redundant, making it harder to find meaningful patterns and relationships.\n",
    "\n",
    "5. **Increased Data Requirements**: To effectively utilize KNN in high-dimensional spaces, we often need significantly more data to cover the increased volume of the feature space adequately. Gathering such large datasets can be impractical or costly.\n",
    "\n",
    "6. **Curse of Proximity**: In high-dimensional spaces, all data points tend to be far apart from each other in terms of Euclidean distance. This means that the concept of \"closeness\" or \"similarity\" becomes less meaningful, as nearly all points are equidistant from each other.\n",
    "\n",
    "To mitigate the curse of dimensionality:\n",
    "\n",
    "1. **Feature Selection/Dimensionality Reduction**: Use techniques like feature selection or dimensionality reduction (e.g., Principal Component Analysis or t-SNE) to reduce the number of irrelevant or redundant features and focus on the most informative ones.\n",
    "\n",
    "2. **Feature Scaling**: Ensure that our features are properly scaled to have similar ranges. Standardization (mean centering and scaling to unit variance) is often recommended.\n",
    "\n",
    "3. **Feature Engineering**: Carefully engineer features to reduce dimensionality while preserving important information.\n",
    "\n",
    "4. **Use Distance Metrics Carefully**: Choose appropriate distance metrics for data and problem domain. Manhattan distance or other custom distance metrics might be more suitable in some cases.\n",
    "\n",
    "5. **Consider Other Algorithms**: In high-dimensional spaces, algorithms that are less sensitive to dimensionality, such as decision trees or linear models, may be more effective than KNN.\n",
    "\n",
    "6. **Collect More Data**: If feasible, gathering more data can help alleviate the sparsity problem associated with high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a1f1f-910c-4ee3-96cb-e9120cedf13d",
   "metadata": {},
   "source": [
    "### How do you handle missing values in KNN?\n",
    "\n",
    "Here are several approaches to deal with missing values in KNN:\n",
    "\n",
    "1. **Remove Instances**: One straightforward approach is to remove instances (rows) from our dataset that contain missing values. This can be a suitable option if the missing values are relatively few and randomly distributed across the dataset. However, if removing instances leads to a significant loss of data, it might not be the best choice.\n",
    "\n",
    "2. **Imputation with Global Mean/Median/Mode**: We can replace missing values with global statistics such as the mean (for numerical features), median (for numerical features with outliers), or mode (for categorical features). This approach helps retain data instances but may not be suitable if there are many missing values or if the data is not missing at random.\n",
    "\n",
    "3. **Imputation with KNN**: One interesting method is to use KNN itself for imputation. For each missing value, we can find the K-nearest neighbors of the data point with the missing value and use the values from those neighbors to impute the missing value. This approach takes into account the local structure of the data.\n",
    "\n",
    "4. **Predictive Modeling**: We can treat missing value imputation as a predictive modeling problem. Create a separate model (e.g., linear regression, decision tree, or another suitable model) to predict the missing values based on the other features in our dataset. Train the model using instances without missing values and then use it to predict the missing values.\n",
    "\n",
    "5. **Interpolation**: For time-series data or ordered data, we can use interpolation techniques (e.g., linear or cubic spline interpolation) to estimate missing values based on neighboring data points in the sequence.\n",
    "\n",
    "6. **Mean/Median Imputation within Groups**: If our data contains groups or clusters, we can replace missing values with the mean or median of the corresponding group. This approach can be more informative than global statistics.\n",
    "\n",
    "7. **Use of Dummy Variable**: For categorical features, we can create a dummy variable (a new category) to represent missing values explicitly. This way, we don't lose information about the absence of data.\n",
    "\n",
    "8. **Data Transformation**: Consider transforming our data to make it less sensitive to missing values. For example, we can use data encoding techniques like one-hot encoding or target encoding, which can handle missing categorical values more gracefully.\n",
    "\n",
    "9. **Multiple Imputation**: Multiple imputation involves creating multiple imputed datasets, each with different imputed values. We then run KNN (or any other algorithm) on each imputed dataset separately and combine the results to obtain more robust predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728622c-f8e7-49e6-8143-a6d2bf4a1711",
   "metadata": {},
   "source": [
    "###  Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "**KNN Classifier**:\n",
    "\n",
    "- **Type of Problem**: KNN classifier is used for classification tasks, where the goal is to assign data points to discrete categories or classes.\n",
    "- **Output**: KNN classifier produces class labels as output.\n",
    "- **Performance Metrics**: Common performance metrics for KNN classification include accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "- **Use Cases**: KNN classification is suitable for problems such as image recognition, spam email detection, sentiment analysis, and medical diagnosis, where the goal is to categorize data into predefined classes.\n",
    "\n",
    "**KNN Regressor**:\n",
    "\n",
    "- **Type of Problem**: KNN regressor is used for regression tasks, where the goal is to predict continuous numerical values.\n",
    "- **Output**: KNN regressor produces numerical values (e.g., real numbers) as output.\n",
    "- **Performance Metrics**: Common performance metrics for KNN regression include mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), and R-squared (R²).\n",
    "- **Use Cases**: KNN regression is suitable for problems such as house price prediction, stock price forecasting, and temperature prediction, where the goal is to estimate a continuous target variable.\n",
    "\n",
    "**Comparison**:\n",
    "\n",
    "1. **Output Type**: The primary difference between KNN classifier and regressor is the type of output they produce. Classifier assigns discrete class labels, while regressor predicts continuous numerical values.\n",
    "\n",
    "2. **Evaluation Metrics**: Each variant has its own set of performance metrics suited to the type of output. Classification uses metrics like accuracy, while regression uses metrics like MAE or RMSE.\n",
    "\n",
    "3. **Nature of Data**: The choice between classifier and regressor depends on the nature of the problem and the data. If the target variable is categorical with distinct classes, classification is appropriate. If the target variable is continuous, regression is suitable.\n",
    "\n",
    "4. **Flexibility**: KNN regressor can be applied to both regression and classification problems by using it with appropriate data and target variable types. KNN classifier, on the other hand, is specifically designed for classification.\n",
    "\n",
    "5. **Decision Boundary**: KNN classifier's decision boundary is often nonlinear and can be complex, while KNN regressor's prediction is a weighted average of nearby points and is inherently smoother.\n",
    "\n",
    "6. **Performance**: The performance of KNN classifier and regressor depends on the specific problem, the choice of distance metric, and the number of neighbors (K). KNN classifier may work well when the decision boundary is not too complex, while KNN regressor can be effective for predicting continuous values when the underlying relationship is relatively smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a8b34-3756-4cec-966b-983af07cd7b2",
   "metadata": {},
   "source": [
    "### What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "**Strengths of KNN**:\n",
    "\n",
    "**1. Simplicity**: KNN is a simple and intuitive algorithm that is easy to understand and implement. It serves as a good baseline model for various machine learning problems.\n",
    "\n",
    "**2. Non-parametric**: KNN is non-parametric, meaning it makes no assumptions about the underlying data distribution. This flexibility allows it to work well with various types of data.\n",
    "\n",
    "**3. Adaptability**: KNN can adapt to complex decision boundaries and handle problems where the class distributions are not well-behaved or linearly separable.\n",
    "\n",
    "**4. Versatility**: KNN can be used for both classification and regression tasks, making it a versatile algorithm.\n",
    "\n",
    "**5. Robust to Outliers**: KNN is relatively robust to outliers because it considers multiple data points when making predictions, which can help mitigate the impact of individual noisy data points.\n",
    "\n",
    "**Weaknesses of KNN**:\n",
    "\n",
    "**1. Computational Complexity**: KNN can be computationally expensive, especially for large datasets and high-dimensional data, as it requires calculating distances between data points.\n",
    "\n",
    "**2. Sensitivity to K Value**: The choice of the number of neighbors (K) can significantly impact the algorithm's performance. Selecting an inappropriate K may lead to overfitting or underfitting.\n",
    "\n",
    "**3. Sensitivity to Feature Scaling**: KNN is sensitive to the scale of features. Features with larger scales can dominate the distance calculations, so it's important to scale or normalize the data.\n",
    "\n",
    "**4. Curse of Dimensionality**: In high-dimensional spaces, KNN's performance can deteriorate due to the \"curse of dimensionality.\" Data points become sparse, and distance calculations become less meaningful.\n",
    "\n",
    "**5. Lack of Interpretability**: KNN does not provide readily interpretable models or feature importance rankings, which can be a limitation in certain applications.\n",
    "\n",
    "**Ways to Address KNN's Weaknesses**:\n",
    "\n",
    "1. **Reduce Dimensionality**: Use dimensionality reduction techniques such as Principal Component Analysis (PCA) or feature selection to reduce the number of features and mitigate the curse of dimensionality.\n",
    "\n",
    "2. **Feature Scaling**: Standardize or normalize the features to ensure that they have similar scales.\n",
    "\n",
    "3. **Cross-Validation**: Employ cross-validation to select an appropriate K value and assess the model's performance more reliably.\n",
    "\n",
    "4. **Distance Metrics**: Choose appropriate distance metrics (e.g., Euclidean, Manhattan, or custom distances) based on the nature of your data and problem domain.\n",
    "\n",
    "5. **Use Distance Weights**: Consider using distance-weighted KNN, where closer neighbors have more influence on predictions, which can help address issues with noisy data or outliers.\n",
    "\n",
    "6. **Ensemble Methods**: Combine multiple KNN models or use ensemble methods like Random Forest or Gradient Boosting to improve predictive performance.\n",
    "\n",
    "7. **Preprocessing**: Address missing values, outliers, and data preprocessing issues before applying KNN.\n",
    "\n",
    "8. **Domain Knowledge**: Incorporate domain knowledge to guide feature selection, distance metric choice, and preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a94cb-240b-4ff2-8743-320c321f5b0d",
   "metadata": {},
   "source": [
    "###  What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "**Euclidean Distance**:\n",
    "- **Formula**: The Euclidean distance between two points (p1 and p2) in a multidimensional space is calculated using the Pythagorean theorem.\n",
    "- **Formula**: d(p1, p2) = √((x1 - x2)² + (y1 - y2)² + ... + (xn - yn)²)\n",
    "- **Geometric Interpretation**: Euclidean distance represents the length of the shortest path (hypotenuse) between two points in a Euclidean space (like a Cartesian plane).\n",
    "- **Properties**: Euclidean distance is sensitive to the magnitude and scale of individual feature dimensions.\n",
    "- **Effect on KNN**: In KNN, Euclidean distance is often used when we want to emphasize the spatial relationships between data points. It tends to work well when features are on similar scales.\n",
    "\n",
    "**Manhattan Distance** (also known as Taxicab or City Block Distance):\n",
    "- **Formula**: The Manhattan distance between two points (p1 and p2) is calculated as the sum of the absolute differences along each dimension.\n",
    "- **Formula**: d(p1, p2) = |x1 - x2| + |y1 - y2| + ... + |xn - yn|\n",
    "- **Geometric Interpretation**: Manhattan distance represents the distance traveled along the grid-like streets of a city (hence the name \"Manhattan\"). It measures the sum of horizontal and vertical distances between two points.\n",
    "- **Properties**: Manhattan distance is less sensitive to outliers and differences in feature scales compared to Euclidean distance.\n",
    "- **Effect on KNN**: In KNN, Manhattan distance is used when we want to emphasize travel along the axes and when we want to reduce the influence of individual feature scales. It can be useful in cases where different dimensions are measured in different units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5b844-bf82-4158-9488-a1229f832f0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What is the role of feature scaling in KNN?\n",
    "\n",
    "The primary purpose of feature scaling in KNN is to ensure that all features have similar scales and contribute equally to the distance calculations. Without proper scaling, features with larger scales can dominate the distance computations, potentially leading to biased results and suboptimal KNN performance.\n",
    "\n",
    "Why feature scaling is important in KNN:\n",
    "\n",
    "1. **Distance-Based Algorithm**: KNN relies on the calculation of distances between data points to determine their similarity. The most common distance metric used is the Euclidean distance, though other metrics like Manhattan distance are also used. These distance metrics are sensitive to the scales of individual features.\n",
    "\n",
    "2. **Impact on Nearest Neighbor Selection**: In KNN, the algorithm identifies the K-nearest neighbors of a data point based on distance calculations. If the scales of features vary widely, features with larger scales can contribute more to the distance, making it more likely that neighbors are selected based on those features rather than others. This can lead to suboptimal neighbor selection and reduced model performance.\n",
    "\n",
    "3. **Magnitude vs. Importance**: Larger values in a feature do not necessarily imply that the feature is more important. Feature scaling helps ensure that the magnitudes of values across different features are not mistaken for their relative importance.\n",
    "\n",
    "Common methods of feature scaling in KNN and other machine learning algorithms include:\n",
    "\n",
    "1. **Min-Max Scaling (Normalization)**: This scales the features to a specific range (typically [0, 1]). It involves subtracting the minimum value of the feature and dividing by the range (maximum - minimum).\n",
    "\n",
    "   Scaled_value = (x - min) / (max - min)\n",
    "\n",
    "2. **Standardization (Z-score Scaling)**: This scales the features to have a mean of 0 and a standard deviation of 1. It involves subtracting the mean of the feature and dividing by the standard deviation.\n",
    "\n",
    "   Scaled_value = (x - mean) / standard_deviation\n",
    "\n",
    "3. **Robust Scaling**: Similar to Min-Max scaling but is based on the interquartile range (IQR) instead of the range (max - min). It is less affected by outliers.\n",
    "\n",
    "   Scaled_value = (x - Q1) / (Q3 - Q1)\n",
    "\n",
    "The choice of scaling method depends on the nature of data and the impact of outliers. Min-Max scaling is useful when we want to constrain our data to a specific range, while standardization is often more appropriate when we want to maintain the data's distribution and are not concerned about a specific range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49111b1-27c8-489f-bd71-dd6c42a7163c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
