{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23cef39-7575-43cf-abf2-559012cda116",
   "metadata": {},
   "source": [
    "### Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "Simple Linear Regression and Multiple Linear Regression are both statistical techniques used for modeling the relationship between one or more independent variables (features) and a dependent variable (target). However, they differ in terms of the number of independent variables they involve and the complexity of the models they create.\n",
    "\n",
    "1. **Simple Linear Regression**:\n",
    "\n",
    "   - **Definition**: Simple Linear Regression is a regression analysis technique that models the relationship between a single independent variable and a dependent variable. It assumes that the relationship between the variables is linear, which means it can be represented as a straight line.\n",
    "   \n",
    "   - **Equation**: The equation for simple linear regression is:\n",
    "     \n",
    "     Y = a + bX\n",
    "     \n",
    "     where:\n",
    "     - Y is the dependent variable.\n",
    "     - X is the independent variable.\n",
    "     - a is the intercept (the value of Y when X is 0).\n",
    "     - b is the slope (the change in Y for a unit change in X).\n",
    "\n",
    "   - **Example**: Let's say you want to predict a student's final exam score (Y) based on the number of hours they spent studying (X). You collect data from several students and create a simple linear regression model to find the relationship. The model will help you predict a student's final exam score based on the number of hours they studied.\n",
    "\n",
    "2. **Multiple Linear Regression**:\n",
    "\n",
    "   - **Definition**: Multiple Linear Regression is an extension of simple linear regression that models the relationship between multiple independent variables and a dependent variable. It assumes a linear relationship but allows for multiple predictors.\n",
    "   \n",
    "   - **Equation**: The equation for multiple linear regression is:\n",
    "     ```\n",
    "     Y = a + b₁X₁ + b₂X₂ + ... + bₙXₙ\n",
    "     ```\n",
    "     where:\n",
    "     - Y is the dependent variable.\n",
    "     - X₁, X₂, ..., Xₙ are the independent variables.\n",
    "     - a is the intercept.\n",
    "     - b₁, b₂, ..., bₙ are the coefficients for each independent variable, representing their respective impact on Y while holding other variables constant.\n",
    "\n",
    "   - **Example**: Suppose you want to predict a house's sale price (Y) based on several features like the number of bedrooms (X₁), square footage (X₂), and the neighborhood's crime rate (X₃). In this case, you collect data on various houses and create a multiple linear regression model to predict the sale price based on all these features. The model considers multiple factors to make more accurate predictions.\n",
    "\n",
    "**Key Differences**:\n",
    "\n",
    "- Simple Linear Regression involves only one independent variable, while Multiple Linear Regression involves two or more independent variables.\n",
    "\n",
    "- Simple Linear Regression models relationships as straight lines, whereas Multiple Linear Regression models relationships as hyperplanes in multidimensional space.\n",
    "\n",
    "- In simple linear regression, we have one coefficient (slope) and one intercept to estimate, while in multiple linear regression, we have multiple coefficients (one for each independent variable) and an intercept to estimate.\n",
    "\n",
    "- Simple linear regression is appropriate when we want to understand the relationship between two variables, whereas multiple linear regression is used when we have more than one predictor variable and want to model the combined effect of those variables on the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98faa6c-c72d-479d-8c57-2aed9c18cc56",
   "metadata": {},
   "source": [
    "###  Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "Linear regression relies on several assumptions that need to hold true for the model to be valid and reliable. Violation of these assumptions can lead to inaccurate or biased results. Here are the key assumptions of linear regression and methods to check whether they hold in a given dataset:\n",
    "\n",
    "1. **Linearity**: The relationship between the independent variables and the dependent variable should be linear. This means that a change in the independent variable(s) should result in a proportional change in the dependent variable.\n",
    "\n",
    "   - **How to check**: Create scatterplots of each independent variable against the dependent variable. If the points on the scatterplot form a roughly straight line, the linearity assumption may hold. Additionally, you can use residual plots to assess linearity.\n",
    "\n",
    "2. **Independence of Errors**: The errors (residuals) should be independent of each other. In other words, the error for one observation should not depend on the error for another observation.\n",
    "\n",
    "   - **How to check**: You can examine a plot of residuals against the order of observation or against time (if applicable) to look for any patterns or trends. A lack of structure in these plots suggests independence of errors.\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the residuals should be constant across all levels of the independent variables. This means that the spread of the residuals should remain roughly the same as you move along the range of the independent variable(s).\n",
    "\n",
    "   - **How to check**: Plot the residuals against the predicted values (fitted values). If the spread of the residuals is roughly constant and there is no obvious funnel shape, homoscedasticity is likely satisfied. You can also use statistical tests like the Breusch-Pagan test or the White test to assess this assumption.\n",
    "\n",
    "4. **Normality of Residuals**: The residuals should follow a normal distribution. This assumption is not required for large sample sizes due to the Central Limit Theorem, but it can be useful for smaller samples.\n",
    "\n",
    "   - **How to check**: Create a histogram or a Q-Q plot of the residuals. If the data is approximately normally distributed, the residuals should follow a bell-shaped curve. You can also use statistical tests like the Shapiro-Wilk test or the Anderson-Darling test to check for normality.\n",
    "\n",
    "5. **No or Little Multicollinearity**: In multiple linear regression, the independent variables should not be highly correlated with each other. High multicollinearity can make it difficult to distinguish the individual effects of predictors.\n",
    "\n",
    "   - **How to check**: Calculate correlation coefficients between pairs of independent variables. If the correlation between two variables is close to 1 or -1, it indicates strong multicollinearity. You can also compute variance inflation factors (VIF) for each independent variable, where values greater than 5 or 10 may suggest multicollinearity.\n",
    "\n",
    "6. **No or Little Outliers and Influential Points**: Outliers and influential data points can have a significant impact on the regression model. Outliers are data points that deviate substantially from the overall pattern, while influential points can unduly influence the model's coefficients.\n",
    "\n",
    "   - **How to check**: Create scatterplots of the data and examine whether there are any data points far from the main cluster. Additionally, you can use diagnostic plots like Cook's distance or leverage plots to identify influential points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8cbb9-8f9f-4638-ad6d-f1f1e9018a24",
   "metadata": {},
   "source": [
    "###  How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "In a linear regression model, the slope and intercept are essential parameters that help us understand and interpret the relationship between the independent variable(s) and the dependent variable. Let's discuss how to interpret the slope and intercept using a real-world scenario.\n",
    "\n",
    "**Scenario**: Suppose we want to understand the relationship between years of work experience (independent variable, X) and annual salary (dependent variable, Y) in a job market. We collected data from a sample of individuals and ran a simple linear regression model.\n",
    "\n",
    "**Linear Regression Equation**:\n",
    "\n",
    "Y = a + bX\n",
    "\n",
    "Here's how to interpret the slope (b) and intercept (a) in this context:\n",
    "\n",
    "1. **Intercept (a)**:\n",
    "   - **Interpretation**: The intercept (a) represents the predicted value of the dependent variable (Y) when the independent variable (X) is zero. In many cases, this interpretation may not make sense in the real world, especially for our scenario.\n",
    "   - **In our example**: The intercept represents the predicted salary for someone with zero years of work experience. However, this interpretation is not meaningful because it's unlikely that someone with zero years of experience would have a salary greater than zero.\n",
    "\n",
    "2. **Slope (b)**:\n",
    "   - **Interpretation**: The slope (b) represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X). In other words, it quantifies the rate of change in Y as X increases by one unit.\n",
    "   - **In our example**: The slope (b) represents how much an individual's salary is expected to increase (or decrease) for each additional year of work experience. If the slope is positive, it means that as experience increases, salary tends to increase. If the slope is negative, it means that as experience increases, salary tends to decrease.\n",
    "\n",
    "For example, if the linear regression model yielded the following results:\n",
    "- Intercept (a) = $40,000\n",
    "- Slope (b) = $2,500\n",
    "\n",
    "We would interpret this as follows:\n",
    "- The predicted salary for someone with zero years of work experience is $40,000 (though this may not be practically meaningful).\n",
    "- For each additional year of work experience, an individual's salary is expected to increase by $2,500 on average.\n",
    "\n",
    "So, if an individual has 5 years of work experience, we can estimate their salary as:\n",
    "\n",
    "Predicted Salary = $40,000 + ($2,500 * 5) = $52,500\n",
    "\n",
    "The intercept represents the starting point (though not always practically meaningful), and the slope represents the change in the dependent variable for a one-unit change in the independent variable, providing valuable insights into the relationship between the variables in your linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649987de-e751-47fa-befe-0b19e2546b15",
   "metadata": {},
   "source": [
    "### Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "**Gradient descent** is an optimization algorithm used in machine learning and deep learning to minimize a cost or loss function and find the best-fitting parameters for a model. It is a foundational technique for training various types of machine learning models, including linear regression, logistic regression, neural networks, and more. The primary goal of gradient descent is to iteratively adjust the model's parameters in the direction that reduces the cost function, eventually converging to the optimal set of parameters.\n",
    "\n",
    "Here's a step-by-step explanation of how gradient descent works and its role in machine learning:\n",
    "\n",
    "1. **Cost or Loss Function**: In machine learning, we typically define a cost or loss function (often denoted as J) that quantifies how well our model's predictions match the actual target values. The goal is to minimize this function.\n",
    "\n",
    "2. **Initialization**: Gradient descent starts by initializing the model's parameters with arbitrary values. These parameters are often denoted as θ, and they represent the coefficients or weights of the model.\n",
    "\n",
    "3. **Gradient Calculation**: The gradient of the cost function with respect to the model parameters (∇J(θ)) is computed. The gradient represents the direction of steepest ascent, meaning it points in the direction where the cost function increases the fastest.\n",
    "\n",
    "4. **Update Parameters**: The model parameters (θ) are updated by subtracting a fraction of the gradient (∇J(θ)) from the current parameter values. This fraction is known as the learning rate (α), and it determines the step size in the parameter space. The update rule is typically written as:\n",
    "   \n",
    "   θ := θ - α * ∇.J(θ)\n",
    "\n",
    "   The learning rate is a crucial hyperparameter that affects the convergence of the algorithm. Choosing an appropriate learning rate is essential, as a too small value can lead to slow convergence, while a too large value can cause the algorithm to overshoot the minimum.\n",
    "\n",
    "5. **Repeat**: Steps 3 and 4 are repeated iteratively for a specified number of iterations or until convergence criteria are met. Convergence criteria can be based on the change in the cost function or the gradient magnitude.\n",
    "\n",
    "6. **Convergence**: Gradient descent converges when it reaches a point where the gradient (∇J(θ)) becomes close to zero or when the cost function no longer decreases significantly. At this point, the algorithm has found the optimal parameters that minimize the cost function.\n",
    "\n",
    "Gradient descent can be used for a wide range of machine learning tasks, including:\n",
    "\n",
    "- **Linear Regression**: In linear regression, gradient descent is used to find the optimal coefficients (slope and intercept) that minimize the mean squared error between predicted and actual values.\n",
    "\n",
    "- **Logistic Regression**: Gradient descent is used to find the optimal weights that maximize the likelihood of the observed data in logistic regression, which is used for binary classification.\n",
    "\n",
    "- **Neural Networks**: In deep learning, gradient descent is the backbone of training neural networks. It adjusts the weights and biases of neurons to minimize the loss function, allowing neural networks to learn complex patterns and representations from data.\n",
    "\n",
    "- **Support Vector Machines (SVMs)**: Gradient descent can be used to train SVMs by minimizing the hinge loss function and finding the optimal hyperplane for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33543f4-1643-40f1-a2dd-b2cb84d57c40",
   "metadata": {},
   "source": [
    "### Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "**Multiple Linear Regression** is a statistical modeling technique used to analyze the relationship between a dependent variable (target) and two or more independent variables (features or predictors). It extends the concepts of simple linear regression, which deals with just one independent variable, to situations where multiple variables influence the outcome.\n",
    "\n",
    "Here's a description of the multiple linear regression model and how it differs from simple linear regression:\n",
    "\n",
    "**Multiple Linear Regression Model**:\n",
    "\n",
    "1. **Model Equation**: The multiple linear regression model can be represented by the following equation:\n",
    "\n",
    "   Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ + ε\n",
    "   \n",
    "   - Y: The dependent variable (target) we want to predict.\n",
    "   - β₀: The intercept, representing the predicted value of Y when all independent variables are zero.\n",
    "   - β₁, β₂, ..., βₙ: The coefficients (slopes) associated with each independent variable, indicating how much a one-unit change in each independent variable affects Y while holding other variables constant.\n",
    "   - X₁, X₂, ..., Xₙ: The independent variables (features).\n",
    "   - ε: The error term, representing the unexplained variability or noise in the model.\n",
    "\n",
    "2. **Assumptions**: Multiple linear regression relies on several assumptions, including linearity, independence of errors, homoscedasticity, and normality of residuals, similar to simple linear regression.\n",
    "\n",
    "3. **Model Interpretation**: In multiple linear regression, the interpretation of coefficients becomes more complex compared to simple linear regression. Each coefficient (β₁, β₂, ..., βₙ) represents the change in the dependent variable (Y) for a one-unit change in the corresponding independent variable (X₁, X₂, ..., Xₙ), while holding all other variables constant. This allows us to assess the individual impact of each feature on the target.\n",
    "\n",
    "**Differences from Simple Linear Regression**:\n",
    "\n",
    "1. **Number of Independent Variables**:\n",
    "   - Simple Linear Regression: Involves only one independent variable.\n",
    "   - Multiple Linear Regression: Involves two or more independent variables.\n",
    "\n",
    "2. **Model Complexity**:\n",
    "   - Simple Linear Regression: Simpler model with one parameter (slope) to estimate.\n",
    "   - Multiple Linear Regression: More complex model with multiple parameters (slopes and an intercept) to estimate.\n",
    "\n",
    "3. **Relationship Complexity**:\n",
    "   - Simple Linear Regression: Assumes a linear relationship between the dependent variable and one independent variable.\n",
    "   - Multiple Linear Regression: Assumes a linear relationship between the dependent variable and a combination of two or more independent variables. This allows for modeling more complex relationships.\n",
    "\n",
    "4. **Interpretation**:\n",
    "   - Simple Linear Regression: Easier to interpret, as there's only one independent variable.\n",
    "   - Multiple Linear Regression: Requires careful interpretation of each coefficient to understand the impact of multiple variables on the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e30233-4f21-4828-b7db-7968ecee4621",
   "metadata": {},
   "source": [
    "###  Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "**Multicollinearity** is a common issue in multiple linear regression when two or more independent variables in the regression model are highly correlated with each other. In other words, it occurs when there is a strong linear relationship between two or more predictor variables. Multicollinearity can complicate the interpretation of the regression model and can lead to unstable and unreliable parameter estimates. Here's a more detailed explanation of multicollinearity and how to detect and address it:\n",
    "\n",
    "**Causes of Multicollinearity**:\n",
    "1. **Data Collection**: Multicollinearity can arise from the way data is collected. For example, if you collect data on both a person's height in inches and their height in centimeters, these two variables will be perfectly correlated because they convey the same information.\n",
    "\n",
    "2. **Inherent Relationships**: In some cases, independent variables are naturally correlated. For example, in finance, variables like income and education level may be highly correlated because higher education often leads to higher income.\n",
    "\n",
    "**Detecting Multicollinearity**:\n",
    "There are several methods to detect multicollinearity:\n",
    "\n",
    "1. **Correlation Matrix**: Calculate the correlation coefficients between all pairs of independent variables. If you find high correlation coefficients (close to 1 or -1), it may indicate multicollinearity.\n",
    "\n",
    "2. **Variance Inflation Factor (VIF)**: Calculate the VIF for each independent variable. The VIF measures how much the variance of an estimated regression coefficient is increased because of multicollinearity. A VIF value greater than 1 suggests some degree of multicollinearity, with higher values indicating a stronger impact.\n",
    "\n",
    "3. **Tolerance**: Tolerance is the reciprocal of the VIF. Low tolerance values (close to 0) are indicative of multicollinearity.\n",
    "\n",
    "4. **Eigenvalues**: You can also examine the eigenvalues of the correlation matrix. If one or more eigenvalues are close to zero, it suggests multicollinearity.\n",
    "\n",
    "**Addressing Multicollinearity**:\n",
    "Once you've detected multicollinearity, you can take several steps to address it:\n",
    "\n",
    "1. **Remove One of the Correlated Variables**: If two or more variables are highly correlated and convey similar information, consider removing one of them from the model. This simplifies the model and reduces multicollinearity.\n",
    "\n",
    "2. **Combine or Transform Variables**: Instead of using highly correlated variables separately, you can create a composite variable or transformation that combines their information. For example, we can create an \"education and experience\" variable if education and experience are correlated.\n",
    "\n",
    "3. **Collect More Data**: In some cases, multicollinearity can be reduced by collecting more data, especially if the issue arises from limited data points.\n",
    "\n",
    "4. **Regularization Techniques**: Techniques like Ridge Regression and Lasso Regression introduce penalty terms to the regression model, which can help reduce the impact of multicollinearity by shrinking coefficient values.\n",
    "\n",
    "5. **Principal Component Analysis (PCA)**: PCA is a dimensionality reduction technique that can be used to create new uncorrelated variables (principal components) from the original correlated variables. We can then use these principal components in your regression model.\n",
    "\n",
    "6. **Partial Correlation**: When using domain knowledge or theory to determine variable importance, we can examine the partial correlation between variables, which measures the strength of the relationship between two variables while controlling for the effects of other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01150d-bf41-4c39-bab1-f2640bee60cc",
   "metadata": {},
   "source": [
    "###  Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "**Polynomial regression** is a variation of linear regression used when the relationship between the independent variable(s) and the dependent variable is not linear but can be better approximated by a polynomial function. While simple linear regression models linear relationships, polynomial regression models nonlinear relationships by using polynomial equations. It allows for a more flexible fit to the data.\n",
    "\n",
    "Here's a description of the polynomial regression model and how it differs from simple linear regression:\n",
    "\n",
    "**Polynomial Regression Model**:\n",
    "\n",
    "1. **Model Equation**: In polynomial regression, the model equation involves polynomial terms of the independent variable(s). The most common form of a polynomial regression model is as follows:\n",
    "\n",
    "   Y = β₀ + β₁X + β₂X² + ... + βₖXᵏ + ε\n",
    "\n",
    "   - Y: The dependent variable (target).\n",
    "   - β₀, β₁, β₂, ..., βₖ: The coefficients or parameters to be estimated.\n",
    "   - X: The independent variable.\n",
    "   - X², X³, ..., Xᵏ: The polynomial terms of X, where k represents the highest degree of the polynomial. Each term represents X raised to a different power.\n",
    "   - ε: The error term, representing the unexplained variability or noise in the model.\n",
    "\n",
    "2. **Nonlinear Relationship**: Polynomial regression allows you to model nonlinear relationships between the independent and dependent variables. By introducing polynomial terms of different degrees, you can capture curved and nonlinear patterns in the data.\n",
    "\n",
    "3. **Degree of Polynomial (k)**: The degree of the polynomial (k) is a hyperparameter that determines the complexity of the polynomial function. Higher values of k allow the model to capture more intricate patterns but also increase the risk of overfitting.\n",
    "\n",
    "**Differences from Simple Linear Regression**:\n",
    "\n",
    "1. **Linearity vs. Nonlinearity**:\n",
    "   - Simple Linear Regression: Models linear relationships, assuming that the relationship between the independent and dependent variables is a straight line.\n",
    "   - Polynomial Regression: Models nonlinear relationships, allowing for curved and nonlinear patterns in the data.\n",
    "\n",
    "2. **Equation Complexity**:\n",
    "   - Simple Linear Regression: Uses a simple linear equation (Y = β₀ + β₁X).\n",
    "   - Polynomial Regression: Uses a more complex equation involving polynomial terms of X (Y = β₀ + β₁X + β₂X² + ... + βₖXᵏ).\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - Simple Linear Regression: Less flexible in capturing nonlinear patterns.\n",
    "   - Polynomial Regression: More flexible and capable of fitting complex, nonlinear data patterns.\n",
    "\n",
    "4. **Interpretation**:\n",
    "   - Simple Linear Regression: Coefficients (β₀ and β₁) represent the slope and intercept of a straight line.\n",
    "   - Polynomial Regression: Interpretation of coefficients becomes more complex as the degree of the polynomial increases, and it may not have straightforward real-world interpretations.\n",
    "\n",
    "5. **Risk of Overfitting**:\n",
    "   - Simple Linear Regression: Simpler models are less prone to overfitting.\n",
    "   - Polynomial Regression: Higher-degree polynomials can be prone to overfitting if not properly regularized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca7ed7-be13-43a2-b28e-3cd9f7a19ba6",
   "metadata": {},
   "source": [
    "###  What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "**Advantages of Polynomial Regression Compared to Linear Regression**:\n",
    "\n",
    "1. **Captures Nonlinear Relationships**: Polynomial regression can model complex and nonlinear relationships between the independent and dependent variables, which linear regression cannot capture effectively. This makes it suitable for situations where the data follows a curved or nonlinear pattern.\n",
    "\n",
    "2. **Higher Accuracy**: When the true relationship between variables is nonlinear, polynomial regression can provide more accurate predictions compared to simple linear regression. It offers a better fit to the data.\n",
    "\n",
    "3. **Increased Flexibility**: By adjusting the degree of the polynomial, you can control the model's flexibility and adapt it to the specific shape of the data, allowing for more customization.\n",
    "\n",
    "4. **Wide Applicability**: Polynomial regression is applicable in various fields, including engineering, physics, biology, economics, and social sciences. It can model a wide range of phenomena that exhibit nonlinear behavior.\n",
    "\n",
    "**Disadvantages of Polynomial Regression Compared to Linear Regression**:\n",
    "\n",
    "1. **Overfitting**: Polynomial regression models with high degrees (e.g., cubic or higher) are prone to overfitting. They can fit the noise in the data rather than the underlying patterns, leading to poor generalization to new data.\n",
    "\n",
    "2. **Interpretation Challenges**: As the degree of the polynomial increases, interpreting the coefficients becomes more challenging and may lack practical real-world meaning.\n",
    "\n",
    "3. **Data Requirement**: Polynomial regression may require a larger amount of data to accurately estimate the higher-degree polynomial coefficients. Small datasets can lead to unstable estimates.\n",
    "\n",
    "4. **Computational Complexity**: Higher-degree polynomial regression can be computationally intensive, both in terms of model estimation and prediction, especially when the degree is very high.\n",
    "\n",
    "**Situations Where Polynomial Regression is Preferred**:\n",
    "\n",
    "1. **Nonlinear Relationships**: When we suspect or observe a nonlinear relationship between the independent and dependent variables, polynomial regression is a better choice. For example, in physics, many physical laws follow nonlinear relationships, making polynomial regression appropriate.\n",
    "\n",
    "2. **Curved Data Patterns**: In situations where the data exhibits curved or curvilinear patterns, such as quadratic, cubic, or higher-order curves, polynomial regression can capture these patterns accurately.\n",
    "\n",
    "3. **Customization**: When we need a flexible model that can adapt to different shapes of data, polynomial regression allows us to tailor the degree of the polynomial to fit the specific data pattern.\n",
    "\n",
    "4. **Exploratory Analysis**: Polynomial regression can be used in exploratory data analysis to better understand the underlying relationships between variables. We can start with a low-degree polynomial and gradually increase it to assess how well the model fits the data.\n",
    "\n",
    "5. **Predictive Accuracy**: When our primary goal is to achieve the highest predictive accuracy, and we are willing to accept some complexity in the model, polynomial regression may be a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2384f6-1fce-452a-8105-677ec1422ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
